# -*- coding: utf-8 -*-
"""customer_segmentation_train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1h9C7j8Os6wLbGW4ADSemFuIOTgzlrNdO

# Custormer Segmentation
---

### Libraries
"""

import os
import pickle
import datetime
import numpy as np
import pandas as pd
import seaborn as sns 
from os.path import join
import matplotlib.pyplot as plt 


from imblearn.over_sampling import SMOTE

from sklearn.preprocessing import \
    LabelEncoder, OneHotEncoder, MinMaxScaler, StandardScaler

from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report

import tensorflow as tf
from tensorflow.keras.callbacks import TensorBoard
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras import Input, Sequential, Model
from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization

#self module
from customer_segmentation_module import \
    EDA, MODEL, cramers_corrected_stat

"""### CONSTANTS"""

CSV_PATH=os.path.join(os.getcwd(),'dataset','train.csv')
MMS_PATH=os.path.join(os.getcwd(),'model','mms.pkl')
OHE_PATH=os.path.join(os.getcwd(), 'model','ohe_encoder.pkl')
MODEL_SAVE_PATH=os.path.join(os.getcwd(),'model','model.h5')

#TENSORBOARD
LOGS_PATH = os.path.join(os.getcwd(),'logs',datetime.datetime.now().strftime('%Y%M%D-%H%M%S'))

"""# 1 Data Loading"""

df = pd.read_csv(CSV_PATH)

df.head(10)

df.describe().T

df.info()

"""# 2 Data Explore & Inspect

#### Overall
"""

#checking the missing values in datasets
df.isna().sum()

#dropping id, and col that has too many NAN and now way to impute
df = df.drop(['id','days_since_prev_campaign_contact'], axis=1)

#check for imbalance datasets
display(df.groupby('term_deposit_subscribed',dropna=False)['term_deposit_subscribed'].size())

"""Just like most of the bank datasets, this is an imbalance datasets. Custormer that don't sign up are 8 times more than custormer that sign up.

### Cat vs Col
"""

#identifying categorical vs continuos data
df.nunique()

display(df['marital'].unique(),
        df['communication_type'].unique(),
        df['prev_campaign_outcome'].unique(),
        df['education'].unique(),)

con = ['customer_age','balance','last_contact_duration',
       'num_contacts_in_campaign',
       'num_contacts_prev_campaign']

cat = df.drop(labels=con, axis=1).columns

"""# 3 Data Visualization"""

sns.palplot(sns.color_palette("inferno", 10))

#quick plots to check data distribution

EDA.displot_graph(con,df,color='k')
EDA.countplot_graph(cat,df,'inferno')

"""# 4 Data Cleaning

### Dealing with Unknown Values
"""

df.nunique()

#checking the ratio of NAN & unknown data
#.count can't include object nan, but .size would
display(df.groupby('marital',dropna=False)['marital'].size(),
        df.groupby('communication_type',dropna=False)['communication_type'].size(),
        df.groupby('prev_campaign_outcome',dropna=False)['prev_campaign_outcome'].size())

"""### Dealing with Missing Values NAN"""

df.isna().sum()

#can we just drop all the rows with NAN?
df_na = df[df.isna().any(axis=1)]

df.describe().T

#comparing the data distribution of rows with NAN vs the main distribution
df_na.describe().T

"""the data distribution of the na columns are within the range of the main distribution, thus, we could savely drop all the rows with na without skewing the main distribution"""

#try to drop everything
df = df.dropna(axis=0)

df.isna().sum()

"""### Duplicated Data"""

df[df.duplicated()].shape
#no duplciated data

"""# 5) Features Selection

## Label Encoding
"""

cat = df[cat].drop(labels='term_deposit_subscribed', axis=1).columns

cat

for i in cat: 
    le = LabelEncoder()
    temp = df[i]
    temp[temp.notnull()]=le.fit_transform(temp[df[i].notnull()])
    df[i]= pd.to_numeric(df[i], errors = 'coerce')
    PICKLE_SAVE_PATH = os.path.join(os.getcwd(),'model',i+'encoder.pkl')
    with open(PICKLE_SAVE_PATH, 'wb') as file: 
        pickle.dump(le,file)
i + '.pkl'

#cat vs con
for i in con:
    lr = LogisticRegression()
    lr.fit(np.expand_dims(df[i],axis=-1),df['term_deposit_subscribed']) #X cont Y cat
    print(lr.score(np.expand_dims(df[i],axis=-1),df['term_deposit_subscribed']))

con

for i in cat:
    print(i)
    matrix = pd.crosstab(df[i],df['term_deposit_subscribed']).to_numpy()
    print(cramers_corrected_stat(matrix))

"""# 5) Data Preprocessing

"""

X = df.drop(['term_deposit_subscribed','marital','default','education','personal_loan'], axis=1)
y = df['term_deposit_subscribed'].astype(int)

mms = MinMaxScaler()
X = mms.fit_transform(X)

MMS_PATH = os.path.join(os.getcwd(),'model','mms.pkl')

with open(MMS_PATH,'wb') as file:
  pickle.dump(mms,file)

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.3, random_state=413)

#display the size of y_train by categories
print("Before SMOTE, size of label '1': {}".format(sum(y_train==1)))
print("Before SMOTE, size of label '0': {} \n".format(sum(y_train==0)))

sm = SMOTE(random_state=13)
X_train, y_train = sm.fit_resample(X_train, y_train)

#X_train_res, y_train_res = sm.fit_sample(X_train, y_train.ravel())

print('After OverSampling, the shape of train_X: {}'.format(X_train.shape))
print('After OverSampling, the shape of train_y: {} \n'.format(y_train.shape))

print("After OverSampling, counts of label '1': {}".format(sum(y_train==1)))
print("After OverSampling, counts of label '0': {}".format(sum(y_train==0)))

ohe = OneHotEncoder(sparse=False)
y_train = ohe.fit_transform(np.expand_dims(y_train,axis=-1))
y_test = ohe.transform(np.expand_dims(y_test,axis=-1))

nb_class = len(np.unique(y_train,axis=0))
input_shape = np.shape(X_train)[1:]

nn = MODEL()
model = nn.nn_model(input_shape,nb_class,128,0.2)

'''
model = Sequential()
model.add(Input(shape=np.shape(X_train)[1:]))
model.add(Dense(128,activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.2))
model.add(Dense(128,activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.2))
model.add(Dense(128,activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.2))
model.add(Dense(nb_class,activation = 'softmax'))
model.summary()
'''

from tensorflow.keras.utils import plot_model
plot_model(model,show_shapes=(True))

model.compile(optimizer='adam',
              loss='categorical_crossentropy', 
              metrics=['accuracy'])

#callbacks
tensorboard_callback = TensorBoard(log_dir=LOGS_PATH,histogram_freq=1)

early_callback = EarlyStopping(monitor='val_loss',patience=4)


hist = model.fit(X_train,y_train,
                 epochs = 50,
                 validation_data=(X_test,y_test),
                 callbacks=[tensorboard_callback,early_callback])

print(hist.history.keys())

plt.figure()
plt.plot(hist.history['loss'])
plt.plot(hist.history['val_loss'])
plt.xlabel('epoch')
plt.legend(['Training Loss','Validation Loss'])
plt.rcParams['figure.figsize'] = (9, 6)
plt.show()

plt.figure()
plt.plot(hist.history['accuracy'])
plt.plot(hist.history['val_accuracy'])
plt.xlabel('epoch')
plt.legend(['Training Acc','Validation Acc'])
plt.rcParams['figure.figsize'] = (9, 6)
plt.show()

y_pred = model.predict(X_test)
y_pred = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)

labels = ['Not Subscribe','Subscribe']
cm = confusion_matrix(y_true,y_pred)
cr = classification_report(y_true,y_pred, target_names = labels)

print(cr)

disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels =labels)
disp.plot(cmap='RdPu')
plt.rcParams['figure.figsize'] = [7, 7]
plt.show()

print(classification_report)

# Commented out IPython magic to ensure Python compatibility.
#loading tensorboard extension
# %load_ext tensorboard

tensorboard --logdir "/content/logs"

"""# Model Saving"""

model.save(MODEL_SAVE_PATH)

#zipping all the file generated from this runtime
!zip -r /content/cusseg.zip /content

from google.colab import files
files.download("/content/cusseg.zip")