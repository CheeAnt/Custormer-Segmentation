# -*- coding: utf-8 -*-
"""github.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1h9C7j8Os6wLbGW4ADSemFuIOTgzlrNdO

# Custormer Segmentation
---

### Libraries
"""

import os
import pickle
from os.path import join

import numpy as np 
import pandas as pd
import seaborn as sns
import scipy.stats as ss
import matplotlib.pyplot as plt

from sklearn.preprocessing import \
    LabelEncoder, MinMaxScaler, StandardScaler

import tensorflow as tf
from tensorflow.keras import Input, Sequential, Model
from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization

from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.experimental import enable_iterative_imputer #need to import this to work
from sklearn.impute import IterativeImputer


#self module
#from customer_segmentation_module import EDA

"""### Functions / Module"""

def cramers_corrected_stat(confusion_matrix):
    """ calculate Cramers V statistic for categorial-categorial association.
        uses correction from Bergsma and Wicher,
        Journal of the Korean Statistical Society 42 (2013): 323-328
    """
    chi2 = ss.chi2_contingency(confusion_matrix)[0]
    n = confusion_matrix.sum()
    phi2 = chi2/n
    r,k = confusion_matrix.shape
    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))  
    rcorr = r - ((r-1)**2)/(n-1)
    kcorr = k - ((k-1)**2)/(n-1)
    return np.sqrt(phi2corr / min( (kcorr-1), (rcorr-1)))

## functions module
class EDA:
  def displot_graph(self,con_col,df):
    #continuos
    for i in con_col:
      plt.figure()
      sns.distplot(df[i])
      plt.show()

  def countplot_graph(self,cat_col,df):
    #categorical
    for i in cat_col:
      plt.figure()
      sns.countplot(df[i])
      plt.show()

"""### PATH"""

#for deep learning later
import datetime
LOGS_PATH = os.path.join(os.getcwd(),'logs',datetime.datetime.now().strftime('%Y%M%D-%H%M%S'))

"""# 1 Data Loading"""

df = pd.read_csv('custormer.csv')
# CSV_PATH = os.path.join(os.getcwd(),'train.csv')

df.head(10)

"""1. communication_type - unknown to change to null? or doesn't matter?

2. day_since_prev_contact --> nan = 0
  corresponding to num_contacts_prev_campaign

3. 
"""

df.describe().T

df.info()

"""# 2 Data Explore & Inspect

#### Overall
"""

df.isna().sum()

df.info()

#checking duplication on ID
df['job_type'].nunique()

df['job_type'].unique()

df.nunique()

"""### Cat vs Col"""

df.info()

#continuos

df = df.drop(labels='id', axis=1)

df.nunique()

display(df['marital'].unique(),
        df['communication_type'].unique(),
        df['prev_campaign_outcome'].unique(),
        df['num_contacts_in_campaign'].unique(),
        df['education'].unique(),)

con = ['customer_age','balance','last_contact_duration',
       'num_contacts_in_campaign','days_since_prev_campaign_contact',
       'num_contacts_prev_campaign']

cat = df.drop(labels=con, axis=1).columns

# draft code
# col = list(df.columns[(df.dtypes=='int64') | (df.dtypes=='float')])
# col.remove('Family_Size')

# #categorical data

# cat = list(df.columns[df.dtypes=='object']) #asigning all the obejc col to cat, first converting it to list
# cat.append('')

"""# Data Viz"""

#quick plots to check data types
for i in con:
  plt.figure()
  sns.distplot(df[i],color='c')
  plt.show()

for i in cat:
  plt.figure()
  sns.countplot(df[i],palette='pastel')
  plt.show()

"""# 3 Data Cleaning

### Converting nan to 0
"""

#checking the relationship between df.days_since_prev_campaign_contact with num_contacts_prev_campaign

df.query('days_since_prev_campaign_contact.isnull() \
          and num_contacts_prev_campaign == 0',engine='python')

df.days_since_prev_campaign_contact.isna().sum()

df.days_since_prev_campaign_contact = df.days_since_prev_campaign_contact.fillna(0)

#succes
df.query('days_since_prev_campaign_contact ==0 \
          and num_contacts_prev_campaign == 0',engine='python')

"""### Dealing with Unknown Values"""

df.nunique()

# display(df.groupby('marital',dropna=False)['marital'].count())

#.count can't include object nan, but .size would

display(df.groupby('marital',dropna=False)['marital'].size(),
        df.groupby('communication_type',dropna=False)['communication_type'].size(),
        df.groupby('prev_campaign_outcome',dropna=False)['prev_campaign_outcome'].size())

"""Decision:
1. Marital - drop nan
2. comm type - remains unknown
3. prev outcome - remains unknown

### Dealing with Missing Values NAN
"""

df.isna().sum()

import missingno as msno

msno.matrix(df)
msno.bar(df)

#Drop
#df.dropna(axis = 0, subset = ['marital'], inplace = True)

#added one class of unknown
df.marital = df["marital"].fillna("unknown")

df.isna().sum()

# display(df.groupby('last_contact_duration',dropna=False)['last_contact_duration'].size(),
#         df.groupby('num_contacts_in_campaign',dropna=False)['num_contacts_in_campaign'].size(),
#         df.groupby('personal_loan',dropna=False)['personal_loan'].size())

df.dropna(axis = 0, subset = ['balance','personal_loan'], inplace = True)

#fill with median and mode for now - review later

for i in con:
  df[i] = df[i].fillna(df[i].median())

for i in cat:
  df[i] = df[i].fillna(df[i].mode()[0])

df.isna().sum()

"""### Duplicated Data"""

df[df.duplicated()].shape

"""# 2)Data Viz - 2.0"""

df.groupby(['Segmentation','Graduated']).agg(['Segmentation','count']).plot(kind='bar')]))
df.groupby(['Segmentation','Graduated']).agg(['Segmentation','count']).plot(kind='bar')]))

# for i in con:
#   plt.figure()
#   sns.distplot(df[i])
#   plt.show()
#writing intot module  
eda = EDA()

eda.distplot_graph(con,df)
eda.countplot_graph(cat,df)

sns.heatmap(all.corr(),annot=True)

# dict to map the binomial data
# temp = df['Ever Married'].map({'Yes':1,'No':0})

"""# 3) Encoder
R
"""

cat = df[cat].drop(labels='term_deposit_subscribed', axis=1).columns

cat

import pickle 

for i in cat: 
    le = LabelEncoder()
    temp = df[i]
    temp[temp.notnull()]=le.fit_transform(temp[df[i].notnull()])
    df[i]= pd.to_numeric(df[i], errors = 'coerce')
    PICKLE_SAVE_PATH = os.path.join(os.getcwd(),i+'encoder.pkl')
    with open(PICKLE_SAVE_PATH, 'wb') as file: 
        pickle.dump(le,file)
i + '.pkl'

"""# 4) Features Selection"""

from sklearn.linear_model import LogisticRegression

#cat vs con
for i in con:
    lr = LogisticRegression()
    lr.fit(np.expand_dims(df[i],axis=-1),df['term_deposit_subscribed']) #X cont Y cat
    print(lr.score(np.expand_dims(df[i],axis=-1),df['term_deposit_subscribed']))

con

import pandas as pd
for i in cat:
    print(i)
    matrix = pd.crosstab(df[i],df['term_deposit_subscribed']).to_numpy()
    print(cramers_corrected_stat(matrix))

#list comprehension method

"""# 5) Data Preprocessing

"""

X = df.drop(['term_deposit_subscribed'], axis=1)
y = df['term_deposit_subscribed'].astype(int)

mms = MinMaxScaler()
X = mms.fit_transform(X)

MMS_PATH = os.path.join(os.getcwd(),'mms.pkl')

with open(MMS_PATH,'wb') as file:
  pickle.dump(mms,file)

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.3, random_state=131)

# from sklearn.preprocessing import OneHotEncoder
# ohe = OneHotEncoder(sparse=False)
# y = ohe.fit_transform(np.array(y).reshape(-1,1)) #converting to array cos pandas dont have this feature

# print("Before OverSampling, counts of label '1': {}".format(sum(y_train==1)))
# print("Before OverSampling, counts of label '0': {} \n".format(sum(y_train==0)))

# from imblearn.over_sampling import SMOTE

# sm = SMOTE(random_state=13)
# X_train, y_train = sm.fit_resample(X_train, y_train)

# #sm = SMOTE(random_state=2)
# #X_train_res, y_train_res = sm.fit_sample(X_train, y_train.ravel())

# print('After OverSampling, the shape of train_X: {}'.format(X_train.shape))
# print('After OverSampling, the shape of train_y: {} \n'.format(y_train.shape))

# print("After OverSampling, counts of label '1': {}".format(sum(y_train==1)))
# print("After OverSampling, counts of label '0': {}".format(sum(y_train==0)))

from sklearn.preprocessing import OneHotEncoder
ohe = OneHotEncoder(sparse=False)
y_train = ohe.fit_transform(np.expand_dims(y_train,axis=-1))
y_test = ohe.transform(np.expand_dims(y_test,axis=-1))

nb_class = len(np.unique(y,axis=0))

model = Sequential()
model.add(Input(shape=np.shape(X_train)[1:]))
model.add(Dense(128,activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.2))
model.add(Dense(128,activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.2))
model.add(Dense(128,activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.2))
model.add(Dense(nb_class,activation = 'softmax'))
model.summary()

from tensorflow.keras.utils import plot_model
plot_model(model,show_shapes=(True))

model.compile(optimizer='adam',
              loss='categorical_crossentropy', 
              metrics=['accuracy'])

hist = model.fit(X_train,y_train,epochs=10,
                validation_data=(X_test,y_test))

print(hist.history.keys())

from tensorflow.keras.callbacks import TensorBoard
from tensorflow.keras.callbacks import EarlyStopping

#callbacks
tensorboard_callback = TensorBoard(log_dir=LOGS_PATH,histogram_freq=1)

early_callback = EarlyStopping(monitor='val_loss',patience=4)


hist = model.fit(X_train,y_train,
                 epochs = 50,
                 validation_data=(X_test,y_test),
                 callbacks=[tensorboard_callback,early_callback])

plt.figure()
plt.plot(hist.history['loss'])
plt.plot(hist.history['val_loss'])
plt.xlabel('epoch')
plt.legend(['Training Loss','Validation Loss'])
plt.rcParams['figure.figsize'] = (9, 6)
plt.show()

plt.figure()
plt.plot(hist.history['accuracy'])
plt.plot(hist.history['val_accuracy'])
plt.xlabel('epoch')
plt.legend(['Training Acc','Validation Acc'])
plt.rcParams['figure.figsize'] = (9, 6)
plt.show()

y_pred = model.predict(X_test)
y_pred = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)

labels = ['Fail to Subscribe','Subscribe']
cm = confusion_matrix(y_true,y_pred)
cr = classification_report(y_true,y_pred, target_names = labels)

print(cr)

disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels =labels)
disp.plot(cmap='YlGnBu')
plt.rcParams['figure.figsize'] = [5, 5]
plt.show()

print(classification_report)

# Commented out IPython magic to ensure Python compatibility.
#loading tensorboard extension
# %load_ext tensorboard

tensorboard --logdir "/content/logs"

"""# Model Saving"""

# OHE_PATH = os.path.join(os.getcwd(),'ohe.pkl')

# with open (OHE_PATH, 'wb') as file:
#   pickle.dump(OHE_PATH,file)

MODEL_SAVE_PATH = os.path.join(os.getcwd(),'model.h5')
model.save(MODEL_SAVE_PATH)



